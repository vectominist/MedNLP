data:
  train_path: data/med_lm.txt

model:
  pretrained_model: ckiplab/albert-tiny-chinese
  # pretrained_model: sentence-transformers/paraphrase-xlm-r-multilingual-v1
train_args:
  output_dir: runs/med_mlm

  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  gradient_accumulation_steps: 1

  learning_rate: 0.00005
  weight_decay: 0.00001
  num_train_epochs: 2
  lr_scheduler_type: linear
  warmup_ratio: 0.0

  save_strategy: epoch
  seed: 7122
  report_to: none
  logging_steps: 1000
  
  dataloader_num_workers: 0

  label_smoothing_factor: 0.1
